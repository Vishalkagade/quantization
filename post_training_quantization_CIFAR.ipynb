{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make torch deterministic\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "fmnist_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(fmnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Load the MNIST test set\n",
    "fmnist_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(fmnist_testset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Define the device\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 32, 32]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "#check the size and shape\n",
    "image, label = next(iter(train_loader))\n",
    "classes = fmnist_trainset.classes\n",
    "print(image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.9894737..2.1308641].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label: horse')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAALfVJREFUeJzt3X9Y1HW+9/EXggwmMIggPwIVf+RvrbUktjJTV7Rdb032XqvtTjdvPbloqact3au0OrsXbXtO2ZZZZ7e07rJaK21rNztlQqdCTdI1M111MTEBxYQBFBD53n94NadJjO8HGT+Az8d1zXXJd9585v2d78DL78zwnhDHcRwBAHCedbDdAADgwkQAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEA4YK1f/9+hYSE6N///d9bbM3c3FyFhIQoNze3Wd8/atQoDR48uMX6AVozAghtysqVKxUSEqItW7bYbgXAOSKAAABWEEDABaa+vl51dXW22wAIILQ/dXV1Wrx4sYYPHy6v16vOnTvrmmuu0YYNG876PY8++qh69OihTp066dprr9WOHTvOqNm1a5d++tOfKjY2VhEREbr88sv1l7/8pcl+jh8/rl27dqmsrMz1PuzcuVPXXXedLrroIl188cV6+OGHz6g5fPiwZsyYoYSEBEVERGjYsGF67rnnAmq+/TrX0qVL1bt3b3k8Hu3cuVOS9Pjjj2vQoEG66KKL1KVLF11++eVatWpVwBpfffWVbrvtNiUkJMjj8WjQoEF69tlnXe8LcDZhthsAWprP59Of/vQn3XTTTZo5c6YqKyv1zDPPKDMzU5s3b9all14aUP/888+rsrJS2dnZqqmp0WOPPabRo0frs88+U0JCgiTp888/11VXXaWLL75YCxcuVOfOnfXnP/9ZkydP1muvvaYbbrjhrP1s3rxZ1113nZYsWaL777+/yf6PHTum8ePHa8qUKfrZz36mV199Vffcc4+GDBmiCRMmSJJOnDihUaNGae/evZozZ47S0tK0evVqTZ8+XeXl5brzzjsD1lyxYoVqamo0a9YseTwexcbG6o9//KPuuOMO/fSnP9Wdd96pmpoabd++XZs2bdLNN98sSSotLdWVV16pkJAQzZkzR/Hx8Xr77bc1Y8YM+Xw+zZs3z/2BAb7LAdqQFStWOJKcTz755Kw19fX1Tm1tbcC2Y8eOOQkJCc5tt93m31ZYWOhIcjp16uQcPHjQv33Tpk2OJGf+/Pn+bWPGjHGGDBni1NTU+Lc1NDQ4P/zhD52+ffv6t23YsMGR5GzYsOGMbUuWLGly/6699lpHkvP888/7t9XW1jqJiYlOVlaWf9vSpUsdSc4LL7zg31ZXV+dkZGQ4kZGRjs/nC9jH6Oho5/DhwwG3NWnSJGfQoEHf28+MGTOcpKQkp6ysLGD7jTfe6Hi9Xuf48eNN7hNwNjwFh3YnNDRU4eHhkqSGhgZ9/fXXqq+v1+WXX65PP/30jPrJkyfr4osv9n89YsQIpaen629/+5sk6euvv9b777+vn/3sZ6qsrFRZWZnKysp09OhRZWZmas+ePfrqq6/O2s+oUaPkOI6rsx9JioyM1C233OL/Ojw8XCNGjNA///lP/7a//e1vSkxM1E033eTf1rFjR91xxx2qqqpSXl5ewJpZWVmKj48P2BYTE6ODBw/qk08+abQPx3H02muvaeLEiXIcx7/fZWVlyszMVEVFRaP3J+AWAYR26bnnntPQoUMVERGhrl27Kj4+Xn/9619VUVFxRm3fvn3P2HbJJZdo//79kqS9e/fKcRzdd999io+PD7gsWbJE0unXY1pKSkqKQkJCArZ16dJFx44d83/95Zdfqm/fvurQIfBHeMCAAf7rvy0tLe2M27nnnnsUGRmpESNGqG/fvsrOztZHH33kv/7IkSMqLy/Xf/7nf56x37/4xS8ktex+48LDa0Bod1544QVNnz5dkydP1q9+9St169ZNoaGhysnJ0b59+4zXa2hokCTdddddyszMbLSmT58+59Tzt4WGhja63XGcZq/ZqVOnM7YNGDBAu3fv1ltvvaV169bptdde05NPPqnFixfrgQce8O/3LbfcomnTpjW67tChQ5vdE0AAod159dVX1atXL73++usBZxLfnK181549e87Y9o9//EM9e/aUJPXq1UvS6ae4xo4d2/INN0OPHj20fft2NTQ0BJwF7dq1y3+9G507d9bUqVM1depU1dXVacqUKfrtb3+rRYsWKT4+XlFRUTp16lSr2W+0LzwFh3bnmzOIb58xbNq0Sfn5+Y3Wr127NuA1nM2bN2vTpk3+d5x169ZNo0aN0tNPP63i4uIzvv/IkSPf209z3obdlOuvv14lJSV65ZVX/Nvq6+v1+OOPKzIyUtdee22Taxw9ejTg6/DwcA0cOFCO4+jkyZMKDQ1VVlaWXnvttUbflt7UfgNN4QwIbdKzzz6rdevWnbH9zjvv1E9+8hO9/vrruuGGG/TjH/9YhYWFeuqppzRw4EBVVVWd8T19+vTR1VdfrdmzZ6u2tlZLly5V165ddffdd/trli1bpquvvlpDhgzRzJkz1atXL5WWlio/P18HDx7U3//+97P2avo2bDdmzZqlp59+WtOnT1dBQYF69uypV199VR999JGWLl2qqKioJtcYN26cEhMTddVVVykhIUFffPGFnnjiCf34xz/2f/9DDz2kDRs2KD09XTNnztTAgQP19ddf69NPP9V7772nr7/+ukX2BxcmAght0vLlyxvdPn36dE2fPl0lJSV6+umn9c4772jgwIF64YUXtHr16kaHhN56663q0KGDli5dqsOHD2vEiBF64oknlJSU5K8ZOHCgtmzZogceeEArV67U0aNH1a1bN1122WVavHhxsHbzrDp16qTc3FwtXLhQzz33nHw+n/r166cVK1Zo+vTprtb4l3/5F7344ot65JFHVFVVpZSUFN1xxx269957/TUJCQnavHmzHnzwQb3++ut68skn1bVrVw0aNEi/+93vgrR3uFCEOOfyyiYAAM3Ea0AAACsIIACAFQQQAMAKAggAYAUBBACwggACAFjR6v4OqKGhQYcOHVJUVNQZAxkBAK2f4ziqrKxUcnLyGQNzv63VBdChQ4eUmppquw0AwDkqKipSSkrKWa9vdQH0zQiQoqIiRUdHW+4GAGDK5/MpNTW1yZFQQQugZcuW6fe//71KSko0bNgwPf744xoxYkST3/fN027R0dEEEAC0YU29jBKUNyG88sorWrBggZYsWaJPP/1Uw4YNU2ZmJh9eBQDwC8osuPT0dF1xxRV64oknJJ1+Y0Fqaqrmzp2rhQsXBtTW1taqtrbW//U3p24VFRWcAQFAG+Tz+eT1epv8Pd7iZ0B1dXUqKCgI+ACrDh06aOzYsY1+HktOTo68Xq//whsQAODC0OIBVFZWplOnTikhISFge0JCgkpKSs6oX7RokSoqKvyXoqKilm4JANAKWX8XnMfjkcfjsd0GAOA8a/EzoLi4OIWGhqq0tDRge2lpqRITE1v65gAAbVSLB1B4eLiGDx+u9evX+7c1NDRo/fr1ysjIaOmbAwC0UUF5Cm7BggWaNm2aLr/8co0YMUJLly5VdXW1fvGLXwTj5gAAbVBQAmjq1Kk6cuSIFi9erJKSEl166aVat27dGW9MAABcuILyd0Dnwu37xwEArZO1vwMCAMANAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFWG2GwAAtH7/MKitclnHGRAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCWXAAgCYlGtT6XNZxBgQAsKLFA+j+++9XSEhIwKV///4tfTMAgDYuKE/BDRo0SO+9997/3EgYz/QBAAIFJRnCwsKUmGjyjCEA4EITlNeA9uzZo+TkZPXq1Us///nPdeDAgbPW1tbWyufzBVwAAO1fiwdQenq6Vq5cqXXr1mn58uUqLCzUNddco8rKykbrc3Jy5PV6/ZfU1NSWbgkA0AqFOI7jBPMGysvL1aNHDz3yyCOaMWPGGdfX1taqtrbW/7XP51NqaqoqKioUHR0dzNYAAC6ZPDfl8/mU6vU2+Xs86O8OiImJ0SWXXKK9e/c2er3H45HH4wl2GwCAVibofwdUVVWlffv2KSkpKdg3BQBoQ1o8gO666y7l5eVp//79+vjjj3XDDTcoNDRUN910U0vfFACgDWvxp+AOHjyom266SUePHlV8fLyuvvpqbdy4UfHx8S19UwCA8+S/C93XHm/8PWdnaPEAevnll1t6SQBAO8QsOACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMCKoH8cA4Bz938f2+i69mBxhdHac+/MdF17neFQ+3f/ss117TWRNUZrdx2T4bp2mNHK0uC43kb1L5a7r3VONv7RNC1hu8sZbN/Izy9yXfv40ytd15466e5YcgYEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWNFqR/E8+tpORVwU6ar27b+96XrdCdf/wKiPG8a5H/eR0iXaaO0Sg9qDxceM1v7kiy9d19ZHJhitXV9uNuqlZ1xn17VVB78yWjsl5WLXtWGdzB7ua1951XXtqJGXGa09dfTVRvXrNue7rv1q1WNGa9dvHe+69q3Kw0Zrf5a/xnXt40YrSybDcv5uuPbfy/YZ1Zv85PcZ8QujtfcVHnVdm9DF/c+aJB075n7tuppq9ws79a7KOAMCAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWhDiO49hu4tt8Pp+8Xq/mz3tQHk+Eq+8Ji3A3d0iS9hR9YtRPWFR317VXZFxntHZifA/XtRFh7u6Lb+yvqnFde/f9y4zWrvvkbaP6effc47p28JBeRmuXV7mfS9czyWzmXWSk+9lxmaOvNVrb1PGPP3Rde0/WT4zWPlLi/j4sM1pZ6mlQ+/8M164zqHU/MfA0s4mEUj+D2vlLnzZae1fhP13XXnrZJUZrR0S5m7cpSSXHQl3X1pw4roVzb1VFRYWio88+KY8zIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYIX7YVfn2fbPdyisY7ir2piYzq7Xral3P89IkupV7bo27OB2o7UzUt3PJuv+w0uN1jZx5/V/DNrakqQj7qd2bf4s32zpI+7nmKVFVhmtHaFa17UHXswxWjvnll8b1a8yqE0yWllyPw1MKjBcu7UI9i86k/Xjwtz/TpGklDD3cx0ja44arV1V/qX74nqDeZQ17nrmDAgAYIVxAH3wwQeaOHGikpOTFRISorVr1wZc7ziOFi9erKSkJHXq1Eljx47Vnj17WqpfAEA7YRxA1dXVGjZsmJYta3yE/8MPP6w//OEPeuqpp7Rp0yZ17txZmZmZqnF5SgYAuDAYPzU6YcIETZgwodHrHMfR0qVLde+992rSpEmSpOeff14JCQlau3atbrzxxnPrFgDQbrToa0CFhYUqKSnR2LFj/du8Xq/S09OVn9/4i8u1tbXy+XwBFwBA+9eiAVRSUiJJSkgIfHdXQkKC/7rvysnJkdfr9V9SU1NbsiUAQCtl/V1wixYtUkVFhf9SVFRkuyUAwHnQogGUmJgoSSotLQ3YXlpa6r/uuzwej6KjowMuAID2r0UDKC0tTYmJiVq/fr1/m8/n06ZNm5SRkdGSNwUAaOOM3wVXVVWlvXv3+r8uLCzUtm3bFBsbq+7du2vevHn6zW9+o759+yotLU333XefkpOTNXny5JbsGwDQxhkH0JYtW3Tdddf5v16wYIEkadq0aVq5cqXuvvtuVVdXa9asWSovL9fVV1+tdevWKSLCYIyDpE//UaQOHdy1FxkZ43rdUaMHGvWhop2uS2deP8No6fABV5r1EiQN9Wb1Lg/L/4hyfwMRRfuMlq7/4lPXtUVVpU0Xfcuuj991XXtwq/uRQJL0vFG1dNyglveRnuk/bjP7E5CfPvuyUf0NtxqsX+8xWjsyrKvr2rSkXkZrR0S4/2Hu07e361pfZZUW3vtgk3XGATRq1Cg5jnPW60NCQvTggw/qwQebvnEAwIXL+rvgAAAXJgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGCF8Sie8+WSrlJYqLvaiCj362Ykms1huiLuEte1rWW2myT99q67XNe+/cFmo7X/z/+aaFRfsrXxT8NtTOHHH5utXeJ+vtsOo5WlIwa1pj9IJrPdcO6WP282221QhNeofsK0W1zXDk5LaLroWw4W/dN17cCRWUZrB0u4y0+25gwIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsKLVjuKpqDmu0FB3s3hi4t3vRljZAaM+ekZebFQfLF/v2WVUf+9//EeQOpH2fvLfRvXHDGrrzVpRg2F9sNTZbgDfa73pA6u+wqh8zYtrXdc+W/aV0dp9+3Z1Xdu/r9kIoZqqo65rL4ro7n7hympXZZwBAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAK1rtLLiEzmEKC3M3C65nmvtZSXs/3mDUxy/zv3Rd+/IPLjFae+79T7uufWLrx0ZrB1Op7QYASQkR7mtLa4LXhyQ98uyfXNeazi9MMPgtXXbE/e8rSUrs4v5OjDOY1Hii1l0tZ0AAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFSGO4zi2m/g2n88nr9dru41mudiw/qugdAG0TYMM62f/zGAE12dHjdZOTI00qt+/p8p17SeFRkurZ5z72ssGmE1XizEYxRNRbzCK56Sjue/WqqKiQtHR0Wet4wwIAGAFAQQAsMI4gD744ANNnDhRycnJCgkJ0dq1awOunz59ukJCQgIu48ePb6l+AQDthHEAVVdXa9iwYVq2bNlZa8aPH6/i4mL/5aWXXjqnJgEA7Y/x5wFNmDBBEyZM+N4aj8ejxMTEZjcFAGj/gvIaUG5urrp166Z+/fpp9uzZOnr07O9Aqa2tlc/nC7gAANq/Fg+g8ePH6/nnn9f69ev1u9/9Tnl5eZowYYJOnTrVaH1OTo68Xq//kpqa2tItAQBaoRb/SO4bb7zR/+8hQ4Zo6NCh6t27t3JzczVmzJgz6hctWqQFCxb4v/b5fIQQAFwAgv427F69eikuLk579+5t9HqPx6Po6OiACwCg/Qt6AB08eFBHjx5VUlJSsG8KANCGGD8FV1VVFXA2U1hYqG3btik2NlaxsbF64IEHlJWVpcTERO3bt0933323+vTpo8zMzBZtHADQthnPgsvNzdV11113xvZp06Zp+fLlmjx5srZu3ary8nIlJydr3Lhx+rd/+zclJCS4Wr85s+DCDWrrjFYGcL48NMSsvr7GfW2Y4X+1DcaeSZLKj7mvPWLQtySZPHmUYjA3TpJiotzXVp1wX3uiXpqfryZnwRmfAY0aNUrfl1nvvPOO6ZIAgAsQs+AAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAK1r884BaymBJoS5rYwzWzTNvBcB5UHbErN5kXNuxKrO1yw3rTca7mf7S7WQwg6280GztkyaNG9zhdS4njHIGBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFjRakfxTP6BFOFyFk+MwV6E5Zv1sd6svE261rDeZASKJH1kWI/2w+SxVVxiuLjBz3296YM2iEx/6UYa1NYY7mdEhPvauHiDPk5J2tl0HWdAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADAilY7C+7wfincZTyWGezFZYlmfaw3nU/VBg0wvE8So8zqJxjMm9rzhdnaHxrMvtpntjQakW5Y/6Mh7mtN57VFxhgUG65dVWVWb9J7zQmztROTDNauMVu7/Jj72pJi97V1De7qOAMCAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArGi1o3hKvpY6uqzdYrDukeY0086dNBw7UmJYf8xgTMmAvmZrDzEY81NWbrb2EYMxJWkDzNauKTOr/80es/pg+d9XGH6DyQiceLOlYwyOvekonsQkk8Wlqir3M3AiIkx/7bpvvszwF1ykwVitmC7ua2tOSfq66TrOgAAAVhBAAAArjAIoJydHV1xxhaKiotStWzdNnjxZu3fvDqipqalRdna2unbtqsjISGVlZam0tLRFmwYAtH1GAZSXl6fs7Gxt3LhR7777rk6ePKlx48apurraXzN//ny9+eabWr16tfLy8nTo0CFNmTKlxRsHALRtRq+GrVu3LuDrlStXqlu3biooKNDIkSNVUVGhZ555RqtWrdLo0aMlSStWrNCAAQO0ceNGXXnllWesWVtbq9raWv/XPp+vOfsBAGhjzuk1oIqKCklSbGysJKmgoEAnT57U2LFj/TX9+/dX9+7dlZ+f3+gaOTk58nq9/ktqauq5tAQAaCOaHUANDQ2aN2+errrqKg0ePFiSVFJSovDwcMXExATUJiQkqKSk8Y8WXbRokSoqKvyXoqKi5rYEAGhDmv13QNnZ2dqxY4c+/PDDc2rA4/HI4/Gc0xoAgLanWWdAc+bM0VtvvaUNGzYoJSXFvz0xMVF1dXUqLy8PqC8tLVViYuI5NQoAaF+MAshxHM2ZM0dr1qzR+++/r7S0tIDrhw8fro4dO2r9+vX+bbt379aBAweUkZHRMh0DANoFo6fgsrOztWrVKr3xxhuKioryv67j9XrVqVMneb1ezZgxQwsWLFBsbKyio6M1d+5cZWRkNPoOOADAhcsogJYvXy5JGjVqVMD2FStWaPr06ZKkRx99VB06dFBWVpZqa2uVmZmpJ598slmNuW0upekSvzjDPkxGcLWmN5CHG9R2NJyTZfrCYZcY97Xl7kdqSZKqDOa1xRjMvZKkmCT3tfWGfZcXm9W3GmYj0lTe+HuPGlVjeh8a1Jo+ZsPCzJoJM7iBsDCzH7gqg9mLNaYz7wx+IcZ0iXRde+KkI+2sbrLO6Lg4jtNkTUREhJYtW6Zly5aZLA0AuMAwCw4AYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYEWzP44h2KokdXRZazJn23Dahw4a1LamUTzXGNTGGYyckSTDaR9G9XHup32cZvAIrj9htrTJKB7TMT+m92FrUWMwFkaSTD7eK97wcRhmMBbI9BddfRDHU5mOmzJZPMbw5yesk/vFqyrdH/wTLu8/zoAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVrXYWXEyo1DHEXW2EwUyoKsNZVpcb1P7VbOmgGpDivrbKcEaa6aMmwu1QP0kGh/I0g15qDPqQpPJyg7UN53u11VlwpsdnwICgtCFJSol3X2s6263+pFl9SYn72qpKs7VTDGbklZeZrV1W7P6OOWKwdp3jro4zIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMCKVjuKx0RUpPvajoZ7vKXcrL61OFbuvnZImtnaZcfM6sNM7nPTR2QQH8Em01vKDfuoN3jMSpIMR0iZ6G1QG2Y4i8fkbjEdl7PxU/e1JmNkJCkp0aw+opP7WtOHbPkRg9pys7WPGDyuTJZ2eyg5AwIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFa02llwNaekUy5rvyhxv26a4Qwu05FdJoYb1A6+zGztmj3ua/cXmq19wnBmV1KS+9pyw5lnJq0YzaSTVFNj0IdBrSQdMXjMBpvJfVhleHxKitzXRhjOmfvEYL6b6Si9csPZcSZqDH9++sS5r60yXHu/Qa3RXD+XdZwBAQCsMAqgnJwcXXHFFYqKilK3bt00efJk7d69O6Bm1KhRCgkJCbjcfvvtLdo0AKDtMwqgvLw8ZWdna+PGjXr33Xd18uRJjRs3TtXV1QF1M2fOVHFxsf/y8MMPt2jTAIC2z+hZ8XXr1gV8vXLlSnXr1k0FBQUaOXKkf/tFF12kxETDD9QAAFxQzuk1oIqKCklSbGxswPYXX3xRcXFxGjx4sBYtWqTjx4+fdY3a2lr5fL6ACwCg/Wv2u+AaGho0b948XXXVVRo8eLB/+80336wePXooOTlZ27dv1z333KPdu3fr9ddfb3SdnJwcPfDAA81tAwDQRjU7gLKzs7Vjxw59+OGHAdtnzZrl//eQIUOUlJSkMWPGaN++ferd+8wP/120aJEWLFjg/9rn8yk1NbW5bQEA2ohmBdCcOXP01ltv6YMPPlBKSsr31qanp0uS9u7d22gAeTweeTye5rQBAGjDjALIcRzNnTtXa9asUW5urtLS0pr8nm3btkmSkkz+GhEA0O4ZBVB2drZWrVqlN954Q1FRUSopOf3n3F6vV506ddK+ffu0atUqXX/99eratau2b9+u+fPna+TIkRo6dGhQdgAA0DYZBdDy5cslnf5j029bsWKFpk+frvDwcL333ntaunSpqqurlZqaqqysLN17770t1jAAoH0IcRzHsd3Et/l8Pnm9XqXLfTqajG263LCfXQa1BYZrX2sQ/79amm209uqHlrmuLT9otLQMRlOdZjBQr8RwaFe5Qa3pC55GM9IM1zYcHafdTZecFybzCyWz42NSK0lHDWqjDdc2/WOQcIPauiCubTgKTg0GtRcbrlus03+qEx199nufWXAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFc3+PKBgC5P75noarFtjuMdxprMtDOQZrH1TYanR2p3SurquLTloMtRE2mhULX1uMKfGZOyIZD7WBOfGdNxUaxHsz1kO5uOwtTzGvwrCmpwBAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAK1rtLLhqSaEuaw1GjSnSsI8gjoIzkv+XV43qd1W6r91k2EswtZa5V2hfog3r44PSxWkmv68kyWwKZNvCGRAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgRasdxdNZ7psrMVh3v+Fsnc8Mak3HfaQZ1JbtMVt7v0FtuNnSGmJYH2dQu99w7RiDWtMHu8lDxWQfm2O/Qe3nwWpC5o/xjga1BtOjJJkdzy6Ga6cY1pv0Um64dk+D2hrDtU1+d5rcJ6ckbXNRxxkQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwotXOgusaKnUMcVfbM9L9umXlzWrHlYgg1scYDhsbb1BbXma2timTB5npDK4Yg8UjDecAdjJYu9Lw4JdXmdWb3C+mc+lM5oHtNly7t0HtWMO1TR625YZrGz5UjGawmfy+kswet6Z9Jxo8xsMMdvKko9MD4ZrAGRAAwAqjAFq+fLmGDh2q6OhoRUdHKyMjQ2+//bb/+pqaGmVnZ6tr166KjIxUVlaWSktLW7xpAEDbZxRAKSkpeuihh1RQUKAtW7Zo9OjRmjRpkj7//PQA+Pnz5+vNN9/U6tWrlZeXp0OHDmnKlClBaRwA0LYZvQY0ceLEgK9/+9vfavny5dq4caNSUlL0zDPPaNWqVRo9erQkacWKFRowYIA2btyoK6+8suW6BgC0ec1+DejUqVN6+eWXVV1drYyMDBUUFOjkyZMaO/Z/Xkrs37+/unfvrvz8/LOuU1tbK5/PF3ABALR/xgH02WefKTIyUh6PR7fffrvWrFmjgQMHqqSkROHh4YqJiQmoT0hIUEnJ2d9nk5OTI6/X67+kpqYa7wQAoO0xDqB+/fpp27Zt2rRpk2bPnq1p06Zp586dzW5g0aJFqqio8F+KioqavRYAoO0w/jug8PBw9enTR5I0fPhwffLJJ3rsscc0depU1dXVqby8POAsqLS0VImJiWddz+PxyOPxmHcOAGjTzvnvgBoaGlRbW6vhw4erY8eOWr9+vf+63bt368CBA8rIyDjXmwEAtDNGZ0CLFi3ShAkT1L17d1VWVmrVqlXKzc3VO++8I6/XqxkzZmjBggWKjY1VdHS05s6dq4yMDN4BBwA4g1EAHT58WLfeequKi4vl9Xo1dOhQvfPOO/rRj34kSXr00UfVoUMHZWVlqba2VpmZmXryySeb1diBU1Koy9rEcvfrxhnOKUkzGLGy56DZ2gPS3NdWnjRbu8SwFxOGU2SMDDA8PnFd3NcW7jFbu9hgrkmJ4Z1iMrpFkmIMag0nvQR1HlehQe2+oHUhXWRYP8Sw3uQ+rDc8+GEGi5uO4ikzeNyarO1iCo8kw8feM888873XR0REaNmyZVq2bJnJsgCACxCz4AAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVgRzCkezOI4jyf0oB0kymVJT12DUjlG96RiMWoO1Tfs2nNxjxHQ/TZjuZ63BA8X0PjGpN71PTB7fpuub7qdpLyacIK5twrSPYD7GTxo2U2dQH8xj35zab36fn02I01TFeXbw4EE+lA4A2oGioiKlpKSc9fpWF0ANDQ06dOiQoqKiFBIS4t/u8/mUmpqqoqIiRUdHW+wwuNjP9uNC2EeJ/WxvWmI/HcdRZWWlkpOT1aHD2V/paXVPwXXo0OF7EzM6OrpdH/xvsJ/tx4WwjxL72d6c6356vd4ma3gTAgDACgIIAGBFmwkgj8ejJUuWyOPx2G4lqNjP9uNC2EeJ/Wxvzud+tro3IQAALgxt5gwIANC+EEAAACsIIACAFQQQAMAKAggAYEWbCaBly5apZ8+eioiIUHp6ujZv3my7pRZ1//33KyQkJODSv39/222dkw8++EATJ05UcnKyQkJCtHbt2oDrHcfR4sWLlZSUpE6dOmns2LHas2ePnWbPQVP7OX369DOO7fjx4+0020w5OTm64oorFBUVpW7dumny5MnavXt3QE1NTY2ys7PVtWtXRUZGKisrS6WlpZY6bh43+zlq1Kgzjuftt99uqePmWb58uYYOHeqfdpCRkaG3337bf/35OpZtIoBeeeUVLViwQEuWLNGnn36qYcOGKTMzU4cPH7bdWosaNGiQiouL/ZcPP/zQdkvnpLq6WsOGDdOyZcsavf7hhx/WH/7wBz311FPatGmTOnfurMzMTNXU1JznTs9NU/spSePHjw84ti+99NJ57PDc5eXlKTs7Wxs3btS7776rkydPaty4caqurvbXzJ8/X2+++aZWr16tvLw8HTp0SFOmTLHYtTk3+ylJM2fODDieDz/8sKWOmyclJUUPPfSQCgoKtGXLFo0ePVqTJk3S559/Luk8HkunDRgxYoSTnZ3t//rUqVNOcnKyk5OTY7GrlrVkyRJn2LBhttsIGknOmjVr/F83NDQ4iYmJzu9//3v/tvLycsfj8TgvvfSShQ5bxnf303EcZ9q0ac6kSZOs9BMshw8fdiQ5eXl5juOcPnYdO3Z0Vq9e7a/54osvHElOfn6+rTbP2Xf303Ec59prr3XuvPNOe00FSZcuXZw//elP5/VYtvozoLq6OhUUFGjs2LH+bR06dNDYsWOVn59vsbOWt2fPHiUnJ6tXr176+c9/rgMHDthuKWgKCwtVUlIScFy9Xq/S09Pb3XGVpNzcXHXr1k39+vXT7NmzdfToUdstnZOKigpJUmxsrCSpoKBAJ0+eDDie/fv3V/fu3dv08fzufn7jxRdfVFxcnAYPHqxFixbp+PHjNtprEadOndLLL7+s6upqZWRknNdj2eqmYX9XWVmZTp06pYSEhIDtCQkJ2rVrl6WuWl56erpWrlypfv36qbi4WA888ICuueYa7dixQ1FRUbbba3ElJSWS1Ohx/ea69mL8+PGaMmWK0tLStG/fPv3617/WhAkTlJ+fr9DQUNvtGWtoaNC8efN01VVXafDgwZJOH8/w8HDFxMQE1Lbl49nYfkrSzTffrB49eig5OVnbt2/XPffco927d+v111+32K25zz77TBkZGaqpqVFkZKTWrFmjgQMHatu2beftWLb6ALpQTJgwwf/voUOHKj09XT169NCf//xnzZgxw2JnOFc33nij/99DhgzR0KFD1bt3b+Xm5mrMmDEWO2ue7Oxs7dixo82/RtmUs+3nrFmz/P8eMmSIkpKSNGbMGO3bt0+9e/c+3202W79+/bRt2zZVVFTo1Vdf1bRp05SXl3dee2j1T8HFxcUpNDT0jHdglJaWKjEx0VJXwRcTE6NLLrlEe/futd1KUHxz7C604ypJvXr1UlxcXJs8tnPmzNFbb72lDRs2BHxuV2Jiourq6lReXh5Q31aP59n2szHp6emS1OaOZ3h4uPr06aPhw4crJydHw4YN02OPPXZej2WrD6Dw8HANHz5c69ev929raGjQ+vXrlZGRYbGz4KqqqtK+ffuUlJRku5WgSEtLU2JiYsBx9fl82rRpU7s+rtLpj50/evRomzq2juNozpw5WrNmjd5//32lpaUFXD98+HB17Ngx4Hju3r1bBw4caFPHs6n9bMy2bdskqU0dz8Y0NDSotrb2/B7LFn1LQ5C8/PLLjsfjcVauXOns3LnTmTVrlhMTE+OUlJTYbq3F/Ou//quTm5vrFBYWOh999JEzduxYJy4uzjl8+LDt1pqtsrLS2bp1q7N161ZHkvPII484W7dudb788kvHcRznoYcecmJiYpw33njD2b59uzNp0iQnLS3NOXHihOXOzXzfflZWVjp33XWXk5+f7xQWFjrvvfee84Mf/MDp27evU1NTY7t112bPnu14vV4nNzfXKS4u9l+OHz/ur7n99tud7t27O++//76zZcsWJyMjw8nIyLDYtbmm9nPv3r3Ogw8+6GzZssUpLCx03njjDadXr17OyJEjLXduZuHChU5eXp5TWFjobN++3Vm4cKETEhLi/Nd//ZfjOOfvWLaJAHIcx3n88ced7t27O+Hh4c6IESOcjRs32m6pRU2dOtVJSkpywsPDnYsvvtiZOnWqs3fvXtttnZMNGzY4ks64TJs2zXGc02/Fvu+++5yEhATH4/E4Y8aMcXbv3m236Wb4vv08fvy4M27cOCc+Pt7p2LGj06NHD2fmzJlt7j9Pje2fJGfFihX+mhMnTji//OUvnS5dujgXXXSRc8MNNzjFxcX2mm6GpvbzwIEDzsiRI53Y2FjH4/E4ffr0cX71q185FRUVdhs3dNtttzk9evRwwsPDnfj4eGfMmDH+8HGc83cs+TwgAIAVrf41IABA+0QAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFb8f5HiDz/T42uVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 3\n",
    "plt.imshow(image[idx].permute(1,2,0), cmap='gray')\n",
    "plt.title(f'Label: {classes[label[idx].item()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets use a pretrained resnet18 model\n",
    "\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "# Modify the model to fit the FashionMNIST dataset\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = model\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5775e-01,  1.2889e-01,  1.7882e+00, -6.6171e-01,  2.1139e-01,\n",
      "         1.2058e+00,  5.7208e-01,  1.1717e+00, -8.6680e-04, -2.1567e+00],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "image, label = next(iter(train_loader))\n",
    "output = net(image.to(device))\n",
    "print(output[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 5000/5000 [03:17<00:00, 25.26it/s, loss=1.64]\n",
      "Epoch 2: 100%|██████████| 5000/5000 [04:05<00:00, 20.40it/s, loss=1.42]\n",
      "Epoch 3: 100%|██████████| 5000/5000 [04:12<00:00, 19.77it/s, loss=1.23]\n",
      "Epoch 4: 100%|██████████| 5000/5000 [04:15<00:00, 19.59it/s, loss=1.11]\n",
      "Epoch 5: 100%|██████████| 5000/5000 [03:49<00:00, 21.79it/s, loss=1.04]\n"
     ]
    }
   ],
   "source": [
    "def train(train_loader, net, epochs:int = 5, total_iterations_limit: int = None):\n",
    "    cross_el = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    total_iterations = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "\n",
    "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "        for data in data_iterator:\n",
    "            num_iterations += 1\n",
    "            total_iterations += 1\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(x)\n",
    "            loss = cross_el(output, y)\n",
    "            loss_sum += loss.item()\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return\n",
    "            \n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
    "    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n",
    "    os.remove('temp_delme.p')\n",
    "\n",
    "MODEL_FILENAME = 'cifar.pt'\n",
    "\n",
    "if Path(MODEL_FILENAME).exists():\n",
    "    net.load_state_dict(torch.load(MODEL_FILENAME))\n",
    "    print('Loaded model from disk')\n",
    "else:\n",
    "    train(train_loader, net, epochs=5)\n",
    "    # Save the model to disk\n",
    "    torch.save(net.state_dict(), MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: nn.Module, total_iterations: int = None) -> None:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    iterations = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc='Testing'):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x)\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct +=1\n",
    "                total +=1\n",
    "            iterations += 1\n",
    "            if total_iterations is not None and iterations >= total_iterations:\n",
    "                break\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print weights and size of the model before quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before quantization\n",
      "tensor([[[-0.0890, -0.1496, -0.1734, -0.0747, -0.0561, -0.0164, -0.0675],\n",
      "         [-0.1811, -0.2734, -0.2770, -0.3027, -0.1897, -0.1288, -0.0349],\n",
      "         [-0.1980, -0.3088, -0.2669, -0.3001, -0.2679, -0.1277, -0.1031],\n",
      "         [-0.1336, -0.1658, -0.1794, -0.2433, -0.2299, -0.2199,  0.0494],\n",
      "         [-0.2054, -0.3313, -0.4007, -0.2993, -0.2495, -0.1555, -0.1460],\n",
      "         [-0.0144, -0.1834, -0.2706, -0.2516, -0.2246, -0.0778, -0.1383],\n",
      "         [ 0.1160, -0.0721, -0.0268, -0.0291, -0.0432, -0.0083, -0.0202]],\n",
      "\n",
      "        [[-0.0396, -0.0844, -0.0092, -0.0431,  0.0220,  0.0784,  0.0919],\n",
      "         [-0.0691, -0.0722, -0.0250,  0.0213,  0.0494,  0.1181,  0.0281],\n",
      "         [ 0.0616, -0.0503,  0.0655, -0.0009,  0.0440,  0.1230,  0.1745],\n",
      "         [ 0.0794, -0.0526,  0.0341,  0.0823,  0.0986,  0.1590,  0.2339],\n",
      "         [-0.1113, -0.1788, -0.0490, -0.0671,  0.0469,  0.0223,  0.0609],\n",
      "         [ 0.0309, -0.0646, -0.0634, -0.0139, -0.0506, -0.0662, -0.0190],\n",
      "         [-0.0078, -0.0122,  0.1102,  0.0337,  0.0065,  0.0834,  0.1510]],\n",
      "\n",
      "        [[ 0.0789,  0.0852,  0.0875,  0.2029,  0.1214,  0.2650,  0.1598],\n",
      "         [ 0.2026,  0.0814,  0.1904,  0.1176,  0.1178,  0.1404,  0.1253],\n",
      "         [ 0.2191,  0.1448,  0.1519,  0.1877,  0.2468,  0.1973,  0.2304],\n",
      "         [ 0.0488,  0.1347,  0.1248,  0.1625,  0.1827,  0.3082,  0.2988],\n",
      "         [-0.0667, -0.0146,  0.1074,  0.0321,  0.1424,  0.1992,  0.1766],\n",
      "         [ 0.1155, -0.0100,  0.0153,  0.0325,  0.1232,  0.1335,  0.0463],\n",
      "         [ 0.1674,  0.0728,  0.2091,  0.2067,  0.1206,  0.1166,  0.1091]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Print the weights matrix of the model before quantization\n",
    "print('Weights before quantization')\n",
    "print(net.conv1.weight[0])\n",
    "print(net.conv1.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model before quantization\n",
      "Size (KB): 44803.585\n"
     ]
    }
   ],
   "source": [
    "print('Size of the model before quantization')\n",
    "print_size_of_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model before quantization: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:06<00:00, 146.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of the model before quantization: ')\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the quantization observers and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishalkagade/.pyenv/versions/3.10.9/lib/python3.10/site-packages/torch/ao/quantization/fx/prepare.py:1536: UserWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (activation_post_process_0): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (conv1): ConvReLU2d(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (activation_post_process_1): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (activation_post_process_2): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (layer1): Module(\n",
       "    (0): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (activation_post_process_3): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_4): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_5): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_6): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_7): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_8): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (layer2): Module(\n",
       "    (0): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (activation_post_process_9): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_10): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_11): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_12): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_13): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_14): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_15): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (layer3): Module(\n",
       "    (0): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (activation_post_process_16): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_17): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_18): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_19): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_20): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_21): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_22): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (layer4): Module(\n",
       "    (0): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (activation_post_process_23): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_24): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_25): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_26): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_27): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_28): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_29): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (activation_post_process_30): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_31): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (activation_post_process_32): HistogramObserver(min_val=inf, max_val=-inf)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input = torch.randn(1, 1, 28, 28) # dummy input for quantization\n",
    "torch.backends.quantized.engine = 'qnnpack' # change it for 'fbgemm' if you want to use that backend\n",
    "qconfig = torch.ao.quantization.get_default_qconfig(torch.backends.quantized.engine) # define the backend configuration\n",
    "model_prepared = prepare_fx(net, {'': qconfig}, example_input) # prepare the model for quantization\n",
    "model_prepared.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a few iterations to calibrate the model\n",
    "with torch.inference_mode():\n",
    "    for i, (x, _) in enumerate(train_loader):\n",
    "        model_prepared(x.to(device))\n",
    "        if i >= 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the prepared model to a quantized model\n",
    "model_quantized = convert_fx(model_prepared).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_quantized.state_dict(), \"model_quantized.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.019438983872532845, zero_point=0, padding=(3, 3))\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Module(\n",
       "    (0): Module(\n",
       "      (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.011495549231767654, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.017344197258353233, zero_point=119, padding=(1, 1))\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.012134271673858166, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.017876815050840378, zero_point=170, padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (layer2): Module(\n",
       "    (0): Module(\n",
       "      (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.012399260886013508, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.024594102054834366, zero_point=119, padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.03241298720240593, zero_point=119)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.0126657010987401, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.022970005869865417, zero_point=218, padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (layer3): Module(\n",
       "    (0): Module(\n",
       "      (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.013384629972279072, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.020787889137864113, zero_point=107, padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.021054109558463097, zero_point=137)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.009305031038820744, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.03341938555240631, zero_point=126, padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (layer4): Module(\n",
       "    (0): Module(\n",
       "      (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.008918891660869122, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.0775541141629219, zero_point=115, padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.030458636581897736, zero_point=116)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.06772592663764954, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.6866560578346252, zero_point=146, padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.21632222831249237, zero_point=213, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Check statistics of the various layers')\n",
    "model_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the dequantized weights and the original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights: \n",
      "Parameter containing:\n",
      "tensor([[[[-8.8984e-02, -1.4963e-01, -1.7343e-01,  ..., -5.6125e-02,\n",
      "           -1.6359e-02, -6.7523e-02],\n",
      "          [-1.8105e-01, -2.7340e-01, -2.7700e-01,  ..., -1.8967e-01,\n",
      "           -1.2884e-01, -3.4930e-02],\n",
      "          [-1.9795e-01, -3.0883e-01, -2.6686e-01,  ..., -2.6791e-01,\n",
      "           -1.2774e-01, -1.0308e-01],\n",
      "          ...,\n",
      "          [-2.0535e-01, -3.3129e-01, -4.0069e-01,  ..., -2.4950e-01,\n",
      "           -1.5552e-01, -1.4603e-01],\n",
      "          [-1.4446e-02, -1.8342e-01, -2.7060e-01,  ..., -2.2460e-01,\n",
      "           -7.7841e-02, -1.3829e-01],\n",
      "          [ 1.1601e-01, -7.2095e-02, -2.6814e-02,  ..., -4.3158e-02,\n",
      "           -8.3160e-03, -2.0210e-02]],\n",
      "\n",
      "         [[-3.9648e-02, -8.4376e-02, -9.2195e-03,  ...,  2.1983e-02,\n",
      "            7.8377e-02,  9.1853e-02],\n",
      "          [-6.9069e-02, -7.2206e-02, -2.4955e-02,  ...,  4.9411e-02,\n",
      "            1.1809e-01,  2.8148e-02],\n",
      "          [ 6.1603e-02, -5.0347e-02,  6.5456e-02,  ...,  4.4012e-02,\n",
      "            1.2299e-01,  1.7445e-01],\n",
      "          ...,\n",
      "          [-1.1126e-01, -1.7880e-01, -4.9028e-02,  ...,  4.6945e-02,\n",
      "            2.2305e-02,  6.0872e-02],\n",
      "          [ 3.0875e-02, -6.4569e-02, -6.3413e-02,  ..., -5.0612e-02,\n",
      "           -6.6208e-02, -1.9012e-02],\n",
      "          [-7.7960e-03, -1.2173e-02,  1.1022e-01,  ...,  6.4559e-03,\n",
      "            8.3397e-02,  1.5097e-01]],\n",
      "\n",
      "         [[ 7.8855e-02,  8.5162e-02,  8.7523e-02,  ...,  1.2139e-01,\n",
      "            2.6496e-01,  1.5976e-01],\n",
      "          [ 2.0259e-01,  8.1371e-02,  1.9039e-01,  ...,  1.1783e-01,\n",
      "            1.4035e-01,  1.2526e-01],\n",
      "          [ 2.1909e-01,  1.4480e-01,  1.5188e-01,  ...,  2.4684e-01,\n",
      "            1.9734e-01,  2.3044e-01],\n",
      "          ...,\n",
      "          [-6.6679e-02, -1.4565e-02,  1.0738e-01,  ...,  1.4241e-01,\n",
      "            1.9920e-01,  1.7662e-01],\n",
      "          [ 1.1555e-01, -9.9680e-03,  1.5348e-02,  ...,  1.2323e-01,\n",
      "            1.3351e-01,  4.6331e-02],\n",
      "          [ 1.6742e-01,  7.2803e-02,  2.0914e-01,  ...,  1.2061e-01,\n",
      "            1.1663e-01,  1.0914e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.5919e-02, -1.3957e-01, -7.4968e-02,  ..., -6.5843e-02,\n",
      "           -6.2038e-02, -3.6188e-02],\n",
      "          [ 5.4909e-03,  2.9467e-02, -5.9844e-02,  ..., -5.0708e-03,\n",
      "            1.8381e-02,  7.8909e-03],\n",
      "          [ 3.9786e-03, -1.1078e-01, -6.9788e-02,  ..., -5.3394e-02,\n",
      "           -1.3169e-01, -1.6934e-01],\n",
      "          ...,\n",
      "          [-9.3219e-02, -1.7270e-01, -1.6436e-01,  ..., -1.4092e-01,\n",
      "           -2.1634e-01, -1.6798e-01],\n",
      "          [-5.2864e-04, -2.7650e-02, -1.6641e-01,  ..., -7.0486e-02,\n",
      "           -1.5780e-01, -1.9830e-01],\n",
      "          [-2.6750e-02, -1.1771e-01, -6.1498e-02,  ..., -6.7879e-02,\n",
      "           -1.7554e-01, -1.2176e-01]],\n",
      "\n",
      "         [[-1.8509e-01, -1.9849e-01, -1.1463e-01,  ..., -1.0026e-01,\n",
      "            2.3664e-02, -8.9707e-02],\n",
      "          [-3.6969e-02,  7.3533e-02,  1.2425e-01,  ...,  3.2562e-03,\n",
      "            2.1085e-02,  9.4862e-03],\n",
      "          [ 5.4473e-02,  3.1734e-03,  3.5098e-02,  ...,  5.6067e-02,\n",
      "           -9.0495e-02, -3.6487e-02],\n",
      "          ...,\n",
      "          [-8.2655e-02, -1.0880e-01, -7.1631e-02,  ..., -2.1680e-01,\n",
      "           -4.4064e-02, -1.2878e-01],\n",
      "          [-4.2737e-02, -1.0764e-01, -1.0369e-01,  ..., -5.7605e-02,\n",
      "           -4.4223e-02,  2.8944e-02],\n",
      "          [ 3.6355e-03, -5.2305e-02,  5.2653e-02,  ..., -4.2281e-02,\n",
      "           -1.0000e-01, -6.8486e-02]],\n",
      "\n",
      "         [[-1.8507e-01, -2.9659e-02, -2.9152e-02,  ...,  1.1760e-01,\n",
      "            1.0164e-01,  5.5785e-02],\n",
      "          [-2.5239e-02,  2.1863e-01,  2.7921e-01,  ...,  3.0689e-01,\n",
      "            3.4392e-01,  1.9746e-01],\n",
      "          [ 1.1785e-01,  3.0750e-01,  3.5610e-01,  ...,  3.3685e-01,\n",
      "            3.4881e-01,  2.9275e-01],\n",
      "          ...,\n",
      "          [ 1.0147e-01,  1.8003e-02,  6.8615e-02,  ...,  7.2995e-02,\n",
      "           -1.7741e-03,  1.4495e-01],\n",
      "          [-4.0222e-02,  2.3065e-02,  8.0801e-03,  ...,  1.0758e-01,\n",
      "            1.6503e-01,  1.3616e-01],\n",
      "          [ 4.7342e-02,  1.7560e-01,  1.6075e-01,  ...,  1.1372e-02,\n",
      "            8.8937e-02,  1.4391e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.8891e-02, -4.9912e-02,  3.3898e-02,  ..., -4.1886e-02,\n",
      "            5.2453e-02, -6.4687e-02],\n",
      "          [-2.0730e-02,  1.5685e-02,  7.7622e-02,  ...,  2.5328e-02,\n",
      "           -7.3963e-02, -5.3054e-02],\n",
      "          [ 5.6819e-02, -6.0185e-02, -2.7535e-02,  ...,  6.3242e-02,\n",
      "           -1.2989e-02,  2.2262e-02],\n",
      "          ...,\n",
      "          [-1.3999e-03, -1.6484e-02, -1.3591e-02,  ...,  3.3439e-02,\n",
      "            4.4242e-02, -3.0352e-02],\n",
      "          [-6.2348e-03,  4.2682e-02,  8.0402e-02,  ..., -1.8956e-02,\n",
      "           -4.9860e-03,  5.6333e-02],\n",
      "          [ 6.8035e-02,  5.9157e-02,  6.2909e-02,  ..., -2.3410e-02,\n",
      "            7.7040e-02,  4.0798e-02]],\n",
      "\n",
      "         [[ 6.1009e-02, -6.2461e-03, -7.1882e-02,  ...,  1.5589e-03,\n",
      "            2.8329e-02,  2.7310e-03],\n",
      "          [ 6.5748e-02, -4.2238e-02,  6.5072e-03,  ...,  7.5275e-02,\n",
      "            5.4083e-02, -4.0611e-02],\n",
      "          [ 5.9026e-02, -1.2862e-02,  1.7253e-02,  ..., -6.7182e-02,\n",
      "            5.4309e-02,  1.3109e-02],\n",
      "          ...,\n",
      "          [ 1.4670e-02, -7.4209e-03, -6.8045e-02,  ...,  5.3867e-02,\n",
      "            1.9729e-02,  4.8191e-02],\n",
      "          [ 4.2154e-03, -2.5377e-02, -7.5701e-02,  ..., -4.6614e-03,\n",
      "           -3.4478e-02,  6.4641e-02],\n",
      "          [ 4.6251e-02, -2.0215e-03,  5.8190e-02,  ...,  4.1600e-02,\n",
      "            3.5939e-02,  1.4404e-02]],\n",
      "\n",
      "         [[-2.5429e-02,  8.4604e-03, -5.9682e-02,  ..., -1.4236e-02,\n",
      "            3.2825e-02, -2.6209e-02],\n",
      "          [-2.3559e-02,  3.0282e-02,  7.7854e-02,  ..., -1.6194e-03,\n",
      "            6.3622e-02,  7.3711e-02],\n",
      "          [-1.0295e-02, -5.0931e-02, -6.0519e-03,  ...,  3.7123e-02,\n",
      "           -6.3315e-02, -4.8337e-02],\n",
      "          ...,\n",
      "          [ 1.9215e-02, -7.2860e-02, -3.7475e-02,  ..., -5.7178e-02,\n",
      "           -5.3031e-02,  1.6092e-02],\n",
      "          [ 7.0682e-02,  2.7819e-02, -7.2464e-02,  ...,  3.0360e-02,\n",
      "           -1.2849e-02,  5.3083e-02],\n",
      "          [ 4.2414e-03, -3.3674e-02,  5.7960e-02,  ..., -7.6714e-02,\n",
      "            1.3491e-02, -6.1970e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 7.1916e-02,  7.6781e-02, -6.4681e-03,  ..., -1.6792e-01,\n",
      "           -1.2881e-01,  1.4775e-02],\n",
      "          [ 2.1594e-01,  9.5777e-02, -2.4165e-03,  ..., -1.2271e-01,\n",
      "           -4.6111e-03,  5.3333e-02],\n",
      "          [ 1.8023e-02, -7.3920e-02, -1.4702e-01,  ..., -1.6630e-01,\n",
      "           -1.6997e-01, -7.1091e-02],\n",
      "          ...,\n",
      "          [-3.3694e-02, -4.8085e-02, -1.9124e-01,  ..., -8.8114e-02,\n",
      "           -2.0253e-02, -8.9735e-02],\n",
      "          [ 5.5994e-02, -5.0043e-02, -5.7292e-02,  ..., -3.3798e-02,\n",
      "            5.6630e-02,  2.0375e-03],\n",
      "          [ 1.3213e-01,  1.3071e-01,  9.7218e-02,  ...,  1.6346e-01,\n",
      "            1.5914e-01,  2.4331e-01]],\n",
      "\n",
      "         [[-2.4388e-02, -1.1464e-03, -1.0333e-02,  ..., -3.4870e-02,\n",
      "           -7.5358e-02, -5.9902e-02],\n",
      "          [-6.0372e-02, -4.5530e-02, -2.1091e-01,  ..., -1.4473e-01,\n",
      "           -6.0821e-02, -3.8346e-02],\n",
      "          [-8.2564e-02, -8.3417e-02, -1.3797e-01,  ..., -8.8937e-02,\n",
      "           -9.2999e-02, -6.0916e-02],\n",
      "          ...,\n",
      "          [-6.1589e-02,  4.8663e-04, -5.4572e-02,  ..., -1.3032e-01,\n",
      "           -1.3951e-01, -1.3599e-01],\n",
      "          [-1.8250e-01, -1.3656e-01, -1.6314e-01,  ..., -2.2087e-01,\n",
      "           -1.2615e-01, -1.3951e-01],\n",
      "          [-6.6480e-02, -1.6860e-01, -1.5897e-01,  ..., -2.5300e-01,\n",
      "           -1.6986e-01, -4.5680e-02]],\n",
      "\n",
      "         [[-9.6199e-02, -2.7425e-02, -7.3393e-02,  ...,  1.5332e-01,\n",
      "            1.4618e-01,  2.0968e-01],\n",
      "          [-2.0958e-01, -8.0346e-02, -8.9828e-02,  ...,  1.8784e-01,\n",
      "            2.1998e-01,  1.6015e-01],\n",
      "          [-7.3195e-02,  1.2574e-01,  1.6126e-01,  ...,  9.5901e-02,\n",
      "            2.3014e-01,  1.2771e-01],\n",
      "          ...,\n",
      "          [ 9.2736e-02,  3.1205e-01,  4.5840e-01,  ...,  3.3053e-01,\n",
      "            2.7884e-01,  1.4087e-01],\n",
      "          [ 7.4784e-02,  1.5101e-01,  1.9693e-01,  ...,  1.6605e-01,\n",
      "            1.4835e-01,  1.1474e-01],\n",
      "          [ 6.7736e-02,  4.8951e-02,  9.7183e-02,  ..., -3.0050e-02,\n",
      "            7.1190e-02,  5.1240e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.7726e-02, -1.2894e-01, -1.4959e-01,  ...,  1.9065e-02,\n",
      "           -5.3082e-04, -1.1283e-01],\n",
      "          [-1.8446e-01, -3.1462e-01, -3.0498e-01,  ..., -3.2910e-01,\n",
      "           -2.5932e-01, -1.9115e-01],\n",
      "          [-1.7951e-01, -2.7908e-01, -3.2866e-01,  ..., -2.0286e-01,\n",
      "           -1.1807e-01, -1.5627e-01],\n",
      "          ...,\n",
      "          [ 1.6410e-01,  2.1878e-01,  2.7399e-01,  ...,  3.0220e-01,\n",
      "            1.9029e-01,  3.5279e-01],\n",
      "          [ 1.9409e-02,  1.4478e-01,  7.7994e-02,  ...,  1.4484e-01,\n",
      "            1.3877e-01,  1.1253e-01],\n",
      "          [ 1.0410e-01,  8.2686e-02,  1.0152e-01,  ...,  2.1721e-02,\n",
      "            1.2731e-01,  2.1170e-01]],\n",
      "\n",
      "         [[ 6.1569e-03, -1.5333e-02,  5.4116e-03,  ...,  1.5385e-02,\n",
      "           -3.4567e-02, -1.0587e-01],\n",
      "          [-8.0464e-02, -1.9268e-01, -1.9635e-01,  ..., -2.4897e-01,\n",
      "           -1.7802e-01, -2.5821e-01],\n",
      "          [-1.7756e-01, -2.2144e-01, -1.9502e-01,  ..., -2.1982e-01,\n",
      "           -8.1478e-02, -7.3317e-02],\n",
      "          ...,\n",
      "          [ 3.6600e-02,  2.3891e-01,  1.9131e-01,  ...,  2.7392e-01,\n",
      "            1.6160e-01,  2.4696e-01],\n",
      "          [ 4.0884e-03,  3.5818e-04,  3.4834e-02,  ..., -3.3328e-02,\n",
      "            6.1429e-02, -7.4636e-03],\n",
      "          [-1.1163e-01,  7.4778e-02,  6.0416e-02,  ..., -7.3096e-02,\n",
      "            4.9405e-03,  5.0625e-02]],\n",
      "\n",
      "         [[ 8.2340e-02,  1.1789e-01,  1.2362e-01,  ...,  3.2422e-02,\n",
      "            7.1319e-02,  1.2286e-02],\n",
      "          [-7.0536e-02, -6.6073e-02, -1.3059e-01,  ..., -1.2081e-01,\n",
      "            4.2919e-03, -1.2203e-01],\n",
      "          [-5.6981e-02, -8.4243e-02, -1.9943e-01,  ..., -1.3231e-01,\n",
      "           -7.7412e-02, -6.0063e-02],\n",
      "          ...,\n",
      "          [ 3.8481e-02,  2.7046e-01,  2.5334e-01,  ...,  3.1957e-01,\n",
      "            2.5828e-01,  1.7824e-01],\n",
      "          [ 4.7939e-03,  9.6365e-02,  6.6421e-02,  ...,  1.5181e-02,\n",
      "            1.8562e-02,  2.2499e-02],\n",
      "          [ 3.9685e-02,  9.0691e-02,  4.2993e-02,  ..., -1.4814e-02,\n",
      "           -2.8061e-02,  6.3759e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 6.2340e-02,  1.0185e-01,  9.6305e-02,  ...,  1.2680e-01,\n",
      "            7.8271e-02,  7.0745e-03],\n",
      "          [-5.2523e-02, -1.8711e-01, -2.0408e-01,  ..., -5.4963e-02,\n",
      "           -1.0049e-02,  8.3779e-02],\n",
      "          [-1.6027e-01, -7.6152e-02, -3.9179e-02,  ...,  1.7868e-01,\n",
      "            1.9207e-01,  3.3583e-02],\n",
      "          ...,\n",
      "          [ 1.4154e-01, -3.9856e-02, -8.5010e-02,  ..., -7.4800e-02,\n",
      "           -4.1212e-01, -1.7790e-01],\n",
      "          [ 6.5013e-02, -5.7976e-02, -1.6692e-01,  ..., -2.5064e-01,\n",
      "           -1.3744e-01,  1.8354e-01],\n",
      "          [-5.1232e-02,  8.3698e-02,  8.1231e-02,  ...,  4.9934e-02,\n",
      "            1.3040e-01,  1.9538e-01]],\n",
      "\n",
      "         [[ 1.4062e-01,  2.9575e-01,  2.4064e-01,  ...,  7.9249e-02,\n",
      "            6.3483e-03, -1.0074e-01],\n",
      "          [-7.5563e-02, -1.2709e-02, -2.2548e-01,  ..., -1.3068e-01,\n",
      "           -1.2559e-01,  5.1243e-03],\n",
      "          [-7.9148e-02, -1.2950e-01,  1.0866e-01,  ...,  2.1135e-01,\n",
      "            2.3467e-01, -2.7747e-02],\n",
      "          ...,\n",
      "          [-9.5126e-03, -2.9187e-02,  3.3615e-03,  ..., -7.7776e-02,\n",
      "           -4.4253e-01, -2.4551e-01],\n",
      "          [ 7.9689e-02, -1.4727e-01, -3.0609e-01,  ..., -3.0947e-01,\n",
      "           -9.6539e-02,  2.7623e-01],\n",
      "          [-8.1997e-02, -3.2285e-02,  4.3103e-02,  ...,  1.9357e-01,\n",
      "            2.0291e-01,  3.0438e-01]],\n",
      "\n",
      "         [[ 3.9625e-02,  1.7203e-01,  1.0321e-01,  ...,  1.5650e-01,\n",
      "            1.4714e-01, -1.6185e-02],\n",
      "          [ 3.0376e-02, -7.2663e-02, -1.1121e-01,  ..., -1.2082e-01,\n",
      "           -5.2294e-02, -7.2613e-02],\n",
      "          [ 7.0179e-03, -4.3928e-02, -3.1155e-02,  ...,  1.3109e-01,\n",
      "            1.8552e-01, -4.5919e-02],\n",
      "          ...,\n",
      "          [ 3.3901e-02, -6.3467e-02, -1.2418e-01,  ..., -2.2742e-01,\n",
      "           -3.8166e-01, -2.0463e-01],\n",
      "          [ 1.7680e-01, -3.2604e-02, -2.2782e-01,  ..., -2.6117e-01,\n",
      "           -1.7516e-01,  2.0532e-01],\n",
      "          [-1.2008e-01, -9.3940e-02,  2.5386e-02,  ...,  1.8009e-01,\n",
      "            1.1319e-01,  2.0780e-01]]]], requires_grad=True)\n",
      "\n",
      "Dequantized weights: \n",
      "tensor([[[[-0.0109, -0.0163, -0.0199,  ..., -0.0054, -0.0018, -0.0072],\n",
      "          [-0.0199, -0.0308, -0.0308,  ..., -0.0217, -0.0145, -0.0036],\n",
      "          [-0.0217, -0.0344, -0.0308,  ..., -0.0308, -0.0145, -0.0109],\n",
      "          ...,\n",
      "          [-0.0235, -0.0380, -0.0453,  ..., -0.0272, -0.0181, -0.0163],\n",
      "          [-0.0018, -0.0199, -0.0308,  ..., -0.0254, -0.0091, -0.0163],\n",
      "          [ 0.0127, -0.0072, -0.0036,  ..., -0.0054, -0.0018, -0.0018]],\n",
      "\n",
      "         [[-0.0036, -0.0091, -0.0018,  ...,  0.0018,  0.0091,  0.0109],\n",
      "          [-0.0072, -0.0072, -0.0036,  ...,  0.0054,  0.0127,  0.0036],\n",
      "          [ 0.0072, -0.0054,  0.0072,  ...,  0.0054,  0.0145,  0.0199],\n",
      "          ...,\n",
      "          [-0.0127, -0.0199, -0.0054,  ...,  0.0054,  0.0018,  0.0072],\n",
      "          [ 0.0036, -0.0072, -0.0072,  ..., -0.0054, -0.0072, -0.0018],\n",
      "          [ 0.0000, -0.0018,  0.0127,  ...,  0.0000,  0.0091,  0.0163]],\n",
      "\n",
      "         [[ 0.0091,  0.0091,  0.0091,  ...,  0.0145,  0.0290,  0.0181],\n",
      "          [ 0.0235,  0.0091,  0.0217,  ...,  0.0127,  0.0163,  0.0145],\n",
      "          [ 0.0254,  0.0163,  0.0163,  ...,  0.0272,  0.0217,  0.0254],\n",
      "          ...,\n",
      "          [-0.0072, -0.0018,  0.0127,  ...,  0.0163,  0.0217,  0.0199],\n",
      "          [ 0.0127, -0.0018,  0.0018,  ...,  0.0145,  0.0145,  0.0054],\n",
      "          [ 0.0181,  0.0091,  0.0235,  ...,  0.0127,  0.0127,  0.0127]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0091, -0.0163, -0.0091,  ..., -0.0072, -0.0072, -0.0036],\n",
      "          [ 0.0000,  0.0036, -0.0072,  ...,  0.0000,  0.0018,  0.0018],\n",
      "          [ 0.0000, -0.0127, -0.0091,  ..., -0.0054, -0.0163, -0.0199],\n",
      "          ...,\n",
      "          [-0.0109, -0.0199, -0.0199,  ..., -0.0163, -0.0254, -0.0199],\n",
      "          [ 0.0000, -0.0036, -0.0199,  ..., -0.0091, -0.0181, -0.0235],\n",
      "          [-0.0036, -0.0145, -0.0072,  ..., -0.0072, -0.0199, -0.0145]],\n",
      "\n",
      "         [[-0.0217, -0.0235, -0.0127,  ..., -0.0109,  0.0036, -0.0109],\n",
      "          [-0.0036,  0.0091,  0.0145,  ...,  0.0000,  0.0018,  0.0018],\n",
      "          [ 0.0072,  0.0000,  0.0036,  ...,  0.0072, -0.0109, -0.0036],\n",
      "          ...,\n",
      "          [-0.0091, -0.0127, -0.0091,  ..., -0.0254, -0.0054, -0.0145],\n",
      "          [-0.0054, -0.0127, -0.0127,  ..., -0.0072, -0.0054,  0.0036],\n",
      "          [ 0.0000, -0.0054,  0.0054,  ..., -0.0054, -0.0109, -0.0072]],\n",
      "\n",
      "         [[-0.0217, -0.0036, -0.0036,  ...,  0.0145,  0.0127,  0.0072],\n",
      "          [-0.0036,  0.0254,  0.0326,  ...,  0.0362,  0.0398,  0.0235],\n",
      "          [ 0.0145,  0.0362,  0.0416,  ...,  0.0398,  0.0416,  0.0344],\n",
      "          ...,\n",
      "          [ 0.0127,  0.0018,  0.0072,  ...,  0.0091,  0.0000,  0.0163],\n",
      "          [-0.0054,  0.0018,  0.0018,  ...,  0.0127,  0.0199,  0.0163],\n",
      "          [ 0.0054,  0.0199,  0.0181,  ...,  0.0018,  0.0109,  0.0163]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0145,  0.0145, -0.0018,  ..., -0.0326, -0.0254,  0.0036],\n",
      "          [ 0.0435,  0.0181,  0.0000,  ..., -0.0235, -0.0018,  0.0109],\n",
      "          [ 0.0036, -0.0145, -0.0290,  ..., -0.0326, -0.0344, -0.0145],\n",
      "          ...,\n",
      "          [-0.0072, -0.0091, -0.0380,  ..., -0.0181, -0.0036, -0.0181],\n",
      "          [ 0.0109, -0.0091, -0.0109,  ..., -0.0072,  0.0109,  0.0000],\n",
      "          [ 0.0254,  0.0254,  0.0199,  ...,  0.0326,  0.0308,  0.0489]],\n",
      "\n",
      "         [[-0.0054,  0.0000, -0.0018,  ..., -0.0072, -0.0145, -0.0127],\n",
      "          [-0.0127, -0.0091, -0.0416,  ..., -0.0290, -0.0127, -0.0072],\n",
      "          [-0.0163, -0.0163, -0.0272,  ..., -0.0181, -0.0181, -0.0127],\n",
      "          ...,\n",
      "          [-0.0127,  0.0000, -0.0109,  ..., -0.0254, -0.0272, -0.0272],\n",
      "          [-0.0362, -0.0272, -0.0326,  ..., -0.0435, -0.0254, -0.0272],\n",
      "          [-0.0127, -0.0326, -0.0308,  ..., -0.0507, -0.0344, -0.0091]],\n",
      "\n",
      "         [[-0.0199, -0.0054, -0.0145,  ...,  0.0308,  0.0290,  0.0416],\n",
      "          [-0.0416, -0.0163, -0.0181,  ...,  0.0380,  0.0435,  0.0326],\n",
      "          [-0.0145,  0.0254,  0.0326,  ...,  0.0199,  0.0453,  0.0254],\n",
      "          ...,\n",
      "          [ 0.0181,  0.0616,  0.0905,  ...,  0.0652,  0.0561,  0.0272],\n",
      "          [ 0.0145,  0.0308,  0.0398,  ...,  0.0326,  0.0290,  0.0235],\n",
      "          [ 0.0127,  0.0091,  0.0199,  ..., -0.0054,  0.0145,  0.0109]]],\n",
      "\n",
      "\n",
      "        [[[-0.0036, -0.0199, -0.0235,  ...,  0.0036,  0.0000, -0.0181],\n",
      "          [-0.0290, -0.0489, -0.0471,  ..., -0.0507, -0.0398, -0.0290],\n",
      "          [-0.0272, -0.0435, -0.0507,  ..., -0.0308, -0.0181, -0.0235],\n",
      "          ...,\n",
      "          [ 0.0254,  0.0326,  0.0416,  ...,  0.0471,  0.0290,  0.0543],\n",
      "          [ 0.0036,  0.0217,  0.0127,  ...,  0.0217,  0.0217,  0.0181],\n",
      "          [ 0.0163,  0.0127,  0.0163,  ...,  0.0036,  0.0199,  0.0326]],\n",
      "\n",
      "         [[ 0.0018, -0.0018,  0.0000,  ...,  0.0018, -0.0054, -0.0163],\n",
      "          [-0.0127, -0.0290, -0.0308,  ..., -0.0380, -0.0272, -0.0398],\n",
      "          [-0.0272, -0.0344, -0.0290,  ..., -0.0344, -0.0127, -0.0109],\n",
      "          ...,\n",
      "          [ 0.0054,  0.0362,  0.0290,  ...,  0.0416,  0.0254,  0.0380],\n",
      "          [ 0.0000,  0.0000,  0.0054,  ..., -0.0054,  0.0091, -0.0018],\n",
      "          [-0.0163,  0.0109,  0.0091,  ..., -0.0109,  0.0000,  0.0072]],\n",
      "\n",
      "         [[ 0.0127,  0.0181,  0.0181,  ...,  0.0054,  0.0109,  0.0018],\n",
      "          [-0.0109, -0.0109, -0.0199,  ..., -0.0181,  0.0000, -0.0181],\n",
      "          [-0.0091, -0.0127, -0.0308,  ..., -0.0199, -0.0127, -0.0091],\n",
      "          ...,\n",
      "          [ 0.0054,  0.0416,  0.0380,  ...,  0.0489,  0.0398,  0.0272],\n",
      "          [ 0.0000,  0.0145,  0.0109,  ...,  0.0018,  0.0036,  0.0036],\n",
      "          [ 0.0054,  0.0145,  0.0072,  ..., -0.0018, -0.0036,  0.0091]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0217,  0.0362,  0.0344,  ...,  0.0435,  0.0272,  0.0018],\n",
      "          [-0.0181, -0.0652, -0.0706,  ..., -0.0199, -0.0036,  0.0290],\n",
      "          [-0.0561, -0.0272, -0.0145,  ...,  0.0616,  0.0670,  0.0109],\n",
      "          ...,\n",
      "          [ 0.0489, -0.0145, -0.0290,  ..., -0.0254, -0.1430, -0.0616],\n",
      "          [ 0.0235, -0.0199, -0.0579,  ..., -0.0869, -0.0471,  0.0634],\n",
      "          [-0.0181,  0.0290,  0.0290,  ...,  0.0181,  0.0453,  0.0688]],\n",
      "\n",
      "         [[ 0.0489,  0.1032,  0.0833,  ...,  0.0272,  0.0018, -0.0344],\n",
      "          [-0.0272, -0.0036, -0.0779,  ..., -0.0453, -0.0435,  0.0018],\n",
      "          [-0.0272, -0.0453,  0.0380,  ...,  0.0742,  0.0815, -0.0091],\n",
      "          ...,\n",
      "          [-0.0036, -0.0109,  0.0018,  ..., -0.0272, -0.1539, -0.0851],\n",
      "          [ 0.0272, -0.0507, -0.1068,  ..., -0.1086, -0.0344,  0.0960],\n",
      "          [-0.0290, -0.0109,  0.0145,  ...,  0.0670,  0.0706,  0.1068]],\n",
      "\n",
      "         [[ 0.0145,  0.0598,  0.0362,  ...,  0.0543,  0.0507, -0.0054],\n",
      "          [ 0.0109, -0.0254, -0.0380,  ..., -0.0416, -0.0181, -0.0254],\n",
      "          [ 0.0018, -0.0145, -0.0109,  ...,  0.0453,  0.0652, -0.0163],\n",
      "          ...,\n",
      "          [ 0.0127, -0.0217, -0.0435,  ..., -0.0797, -0.1340, -0.0706],\n",
      "          [ 0.0616, -0.0109, -0.0797,  ..., -0.0905, -0.0616,  0.0724],\n",
      "          [-0.0416, -0.0326,  0.0091,  ...,  0.0634,  0.0398,  0.0724]]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original weights: ')\n",
    "print(net.conv1.weight)  # Display only a small part of the weights for clarity\n",
    "print('')\n",
    "print('Dequantized weights: ')\n",
    "print(torch.dequantize(model_quantized.conv1.weight()))  # Display only a small part of the dequantized weights for clarity\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print size and accuracy of the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model after quantization\n",
      "Size (KB): 11223.149\n"
     ]
    }
   ],
   "source": [
    "print('Size of the model after quantization')\n",
    "print_size_of_model(model_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model after quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:09<00:00, 100.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing the model after quantization')\n",
    "test(model_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
