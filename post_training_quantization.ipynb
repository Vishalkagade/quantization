{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make torch deterministic\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.2861,), (0.3530,))])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "fmnist_trainset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(fmnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Load the MNIST test set\n",
    "fmnist_testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(fmnist_testset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Define the device\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "#check the size and shape\n",
    "image, label = next(iter(train_loader))\n",
    "classes = fmnist_trainset.classes\n",
    "print(image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label: Shirt')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 1\n",
    "plt.imshow(image[idx].permute(1,2,0), cmap='gray')\n",
    "plt.title(f'Label: {classes[label[idx].item()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets use a pretrained resnet18 model\n",
    "\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "# Modify the model to fit the FashionMNIST dataset\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = model\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5611, -1.5845,  1.0030,  1.0037, -1.8422,  0.1038,  1.2024, -0.0512,\n",
      "         0.6701,  0.1114], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "image, label = next(iter(train_loader))\n",
    "output = net(image.to(device))\n",
    "print(output[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "def train(train_loader, net, epochs:int = 5, total_iterations_limit: int = None):\n",
    "    cross_el = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    total_iterations = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "\n",
    "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "        for data in data_iterator:\n",
    "            num_iterations += 1\n",
    "            total_iterations += 1\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(x)\n",
    "            loss = cross_el(output, y)\n",
    "            loss_sum += loss.item()\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return\n",
    "            \n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
    "    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n",
    "    os.remove('temp_delme.p')\n",
    "\n",
    "MODEL_FILENAME = 'smnist_ptq.pt'\n",
    "\n",
    "if Path(MODEL_FILENAME).exists():\n",
    "    net.load_state_dict(torch.load(MODEL_FILENAME))\n",
    "    print('Loaded model from disk')\n",
    "else:\n",
    "    train(train_loader, net, epochs=1)\n",
    "    # Save the model to disk\n",
    "    torch.save(net.state_dict(), MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: nn.Module, total_iterations: int = None) -> None:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    iterations = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc='Testing'):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x)\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct +=1\n",
    "                total +=1\n",
    "            iterations += 1\n",
    "            if total_iterations is not None and iterations >= total_iterations:\n",
    "                break\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print weights and size of the model before quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before quantization\n",
      "tensor([[[-0.0849, -0.1300, -0.0509,  0.0927, -0.0406, -0.0344,  0.0132],\n",
      "         [ 0.0516,  0.1093, -0.0853, -0.0719, -0.1049,  0.0262, -0.1976],\n",
      "         [-0.0052,  0.0422, -0.0612,  0.1085,  0.1315, -0.1772, -0.1539],\n",
      "         [-0.1047, -0.0052, -0.0385,  0.1363,  0.0843, -0.1121, -0.1579],\n",
      "         [-0.0150,  0.0024,  0.1148,  0.1397,  0.0339, -0.1615, -0.2500],\n",
      "         [ 0.0935, -0.0800,  0.0848,  0.0522,  0.0980, -0.0620, -0.0535],\n",
      "         [-0.0837,  0.0536, -0.1011,  0.1049,  0.0879, -0.0275,  0.0883]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Print the weights matrix of the model before quantization\n",
    "print('Weights before quantization')\n",
    "print(net.conv1.weight[0])\n",
    "print(net.conv1.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model before quantization\n",
      "Size (KB): 44778.497\n"
     ]
    }
   ],
   "source": [
    "print('Size of the model before quantization')\n",
    "print_size_of_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model before quantization: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:06<00:00, 154.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of the model before quantization: ')\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the quantization observers and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishalkagade/.pyenv/versions/3.10.9/lib/python3.10/site-packages/torch/ao/quantization/fx/prepare.py:1536: UserWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (activation_post_process_0): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (conv1): ConvReLU2d(\n",
       "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (activation_post_process_1): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (activation_post_process_2): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (layer1): Module(\n",
       "    (0): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (activation_post_process_3): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_4): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_5): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_6): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_7): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_8): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (layer2): Module(\n",
       "    (0): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (activation_post_process_9): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_10): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_11): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_12): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_13): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_14): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_15): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (layer3): Module(\n",
       "    (0): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (activation_post_process_16): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_17): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_18): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_19): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_20): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_21): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_22): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (layer4): Module(\n",
       "    (0): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (activation_post_process_23): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_24): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_25): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_26): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_27): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_28): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_29): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (activation_post_process_30): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_31): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (activation_post_process_32): HistogramObserver(min_val=inf, max_val=-inf)\n",
       ")"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input = torch.randn(1, 1, 28, 28) # dummy input for quantization\n",
    "torch.backends.quantized.engine = 'qnnpack' # change it for 'fbgemm' if you want to use that backend\n",
    "qconfig = torch.ao.quantization.get_default_qconfig(torch.backends.quantized.engine) # define the backend configuration\n",
    "model_prepared = prepare_fx(net, {'': qconfig}, example_input) # prepare the model for quantization\n",
    "model_prepared.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a few iterations to calibrate the model\n",
    "with torch.inference_mode():\n",
    "    for i, (x, _) in enumerate(train_loader):\n",
    "        model_prepared(x.to(device))\n",
    "        if i >= 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the prepared model to a quantized model\n",
    "model_quantized = convert_fx(model_prepared).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_quantized.state_dict(), \"model_quantized.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (conv1): QuantizedConvReLU2d(1, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.01053610723465681, zero_point=0, padding=(3, 3))\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Module(\n",
       "    (0): Module(\n",
       "      (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.005437065847218037, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.01702350564301014, zero_point=131, padding=(1, 1))\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.005541318561881781, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.02067071571946144, zero_point=136, padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (layer2): Module(\n",
       "    (0): Module(\n",
       "      (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.007975183427333832, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.01963534951210022, zero_point=127, padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.015078023076057434, zero_point=114)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.007371739484369755, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.016092076897621155, zero_point=156, padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (layer3): Module(\n",
       "    (0): Module(\n",
       "      (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.006822084076702595, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.014664840884506702, zero_point=116, padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.008437169715762138, zero_point=138)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.004886067938059568, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.01400348823517561, zero_point=180, padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (layer4): Module(\n",
       "    (0): Module(\n",
       "      (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.0027936515398323536, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.009958016686141491, zero_point=170, padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.01368003524839878, zero_point=117)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.004358763340860605, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.04547358676791191, zero_point=116, padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.16762125492095947, zero_point=174, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Check statistics of the various layers')\n",
    "model_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the dequantized weights and the original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights: \n",
      "Parameter containing:\n",
      "tensor([[[[-8.4941e-02, -1.2996e-01, -5.0867e-02,  ..., -4.0584e-02,\n",
      "           -3.4440e-02,  1.3186e-02],\n",
      "          [ 5.1589e-02,  1.0929e-01, -8.5269e-02,  ..., -1.0493e-01,\n",
      "            2.6219e-02, -1.9757e-01],\n",
      "          [-5.2159e-03,  4.2154e-02, -6.1159e-02,  ...,  1.3154e-01,\n",
      "           -1.7724e-01, -1.5392e-01],\n",
      "          ...,\n",
      "          [-1.4976e-02,  2.3904e-03,  1.1481e-01,  ...,  3.3922e-02,\n",
      "           -1.6151e-01, -2.4998e-01],\n",
      "          [ 9.3454e-02, -7.9977e-02,  8.4802e-02,  ...,  9.8006e-02,\n",
      "           -6.1979e-02, -5.3476e-02],\n",
      "          [-8.3698e-02,  5.3649e-02, -1.0112e-01,  ...,  8.7942e-02,\n",
      "           -2.7515e-02,  8.8286e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5470e-02, -1.1542e-02,  1.5714e-01,  ...,  1.1039e-04,\n",
      "           -1.3876e-01, -2.2174e-02],\n",
      "          [-9.1518e-02,  2.7622e-02, -5.5239e-02,  ..., -6.4452e-02,\n",
      "           -1.4995e-01,  1.0194e-01],\n",
      "          [-6.6572e-02,  8.0790e-03,  4.7098e-03,  ...,  6.3358e-02,\n",
      "           -4.5280e-02, -1.3062e-01],\n",
      "          ...,\n",
      "          [ 4.2171e-02, -9.4665e-02, -9.4981e-02,  ..., -4.5781e-03,\n",
      "            9.8315e-03, -9.4601e-02],\n",
      "          [ 1.1672e-01,  7.7785e-02, -8.1429e-02,  ..., -8.6431e-02,\n",
      "            2.3227e-02, -1.9719e-02],\n",
      "          [-4.9678e-02,  1.1907e-01,  2.2762e-02,  ...,  8.4302e-02,\n",
      "            1.2020e-01, -4.7711e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.9676e-02,  1.5088e-02,  8.6035e-02,  ..., -1.1694e-01,\n",
      "            9.1467e-02,  9.0343e-02],\n",
      "          [ 6.1374e-02,  5.9724e-02, -1.1918e-01,  ..., -1.1309e-01,\n",
      "            1.2830e-01,  1.3104e-01],\n",
      "          [ 2.5559e-03,  9.8730e-02,  2.3849e-02,  ..., -2.6065e-02,\n",
      "           -1.0105e-01,  1.1945e-02],\n",
      "          ...,\n",
      "          [ 4.9307e-02, -1.3746e-01, -9.1701e-02,  ...,  1.2736e-01,\n",
      "            8.3839e-02, -4.0575e-03],\n",
      "          [-3.5271e-03, -5.0854e-02, -1.2850e-01,  ..., -6.2479e-03,\n",
      "           -6.9760e-02,  4.5291e-02],\n",
      "          [-1.2797e-01,  3.4254e-02,  2.8422e-02,  ..., -9.1047e-02,\n",
      "           -5.7970e-02, -6.3628e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.2235e-02,  1.9711e-02,  1.4899e-01,  ...,  2.6649e-02,\n",
      "           -9.8883e-02, -9.8618e-03],\n",
      "          [-4.5510e-02, -1.1066e-03,  1.6873e-01,  ..., -2.0212e-02,\n",
      "            1.6343e-03, -8.8785e-02],\n",
      "          [ 1.0834e-01,  2.9915e-02,  5.9758e-03,  ...,  3.4760e-02,\n",
      "           -1.9291e-01, -1.6604e-01],\n",
      "          ...,\n",
      "          [-5.8124e-02,  1.2892e-01,  9.8319e-02,  ..., -4.0118e-02,\n",
      "           -2.7521e-02,  1.3233e-03],\n",
      "          [-6.2072e-02, -5.6673e-02,  1.4704e-01,  ..., -6.3848e-02,\n",
      "           -5.0197e-02, -5.2951e-02],\n",
      "          [ 9.8850e-03, -6.9067e-03, -7.7058e-02,  ..., -1.6640e-01,\n",
      "           -1.1519e-01, -1.1985e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.7999e-02, -5.6730e-02, -7.4822e-02,  ..., -8.1654e-02,\n",
      "           -2.2814e-01,  8.1095e-02],\n",
      "          [-2.5131e-02, -2.4946e-01, -2.0361e-01,  ..., -1.9352e-01,\n",
      "           -1.4847e-01,  1.0941e-01],\n",
      "          [ 9.9587e-02, -5.7466e-02, -6.2988e-02,  ..., -1.9965e-03,\n",
      "            1.3581e-01, -1.7035e-01],\n",
      "          ...,\n",
      "          [ 4.3255e-02, -1.1322e-01,  2.7955e-02,  ...,  1.3248e-01,\n",
      "           -1.4201e-01,  1.0160e-01],\n",
      "          [ 9.1568e-02,  5.5995e-02,  2.4286e-02,  ...,  2.9879e-02,\n",
      "           -1.7210e-01,  7.9463e-02],\n",
      "          [-5.5731e-02, -4.8310e-02, -1.0980e-01,  ..., -4.8737e-02,\n",
      "           -3.7358e-02,  1.9828e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.9156e-02,  1.4983e-01,  1.1171e-01,  ...,  9.3520e-02,\n",
      "           -2.2870e-02, -9.7826e-02],\n",
      "          [ 1.3251e-01,  6.9171e-02, -1.1458e-01,  ...,  7.3824e-02,\n",
      "            1.1843e-02,  1.1996e-01],\n",
      "          [-5.2886e-03, -1.0324e-01, -9.9772e-02,  ...,  1.5698e-02,\n",
      "            2.8432e-02,  1.6093e-01],\n",
      "          ...,\n",
      "          [ 5.1958e-02, -4.5042e-02, -1.9028e-01,  ...,  9.1781e-02,\n",
      "            3.0629e-02,  6.4234e-02],\n",
      "          [-5.1348e-02,  4.8844e-02, -7.1643e-02,  ...,  5.5887e-02,\n",
      "            3.0025e-02, -7.3340e-02],\n",
      "          [ 1.1838e-01, -3.2268e-03,  8.2572e-02,  ...,  1.5457e-01,\n",
      "           -8.3538e-02, -5.2209e-02]]]], requires_grad=True)\n",
      "\n",
      "Dequantized weights: \n",
      "tensor([[[[-0.0372, -0.0559, -0.0223,  ..., -0.0168, -0.0149,  0.0056],\n",
      "          [ 0.0223,  0.0466, -0.0372,  ..., -0.0447,  0.0112, -0.0838],\n",
      "          [-0.0019,  0.0186, -0.0261,  ...,  0.0559, -0.0764, -0.0652],\n",
      "          ...,\n",
      "          [-0.0056,  0.0019,  0.0484,  ...,  0.0149, -0.0689, -0.1080],\n",
      "          [ 0.0391, -0.0335,  0.0372,  ...,  0.0428, -0.0261, -0.0223],\n",
      "          [-0.0354,  0.0223, -0.0428,  ...,  0.0372, -0.0112,  0.0372]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0074, -0.0037,  0.0410,  ...,  0.0000, -0.0354, -0.0056],\n",
      "          [-0.0242,  0.0074, -0.0149,  ..., -0.0168, -0.0391,  0.0261],\n",
      "          [-0.0168,  0.0019,  0.0019,  ...,  0.0168, -0.0112, -0.0335],\n",
      "          ...,\n",
      "          [ 0.0112, -0.0242, -0.0242,  ..., -0.0019,  0.0019, -0.0242],\n",
      "          [ 0.0298,  0.0205, -0.0205,  ..., -0.0223,  0.0056, -0.0056],\n",
      "          [-0.0130,  0.0317,  0.0056,  ...,  0.0223,  0.0317, -0.0130]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0093,  0.0056,  0.0391,  ...,  0.0074, -0.0261, -0.0019],\n",
      "          [-0.0112,  0.0000,  0.0447,  ..., -0.0056,  0.0000, -0.0242],\n",
      "          [ 0.0279,  0.0074,  0.0019,  ...,  0.0093, -0.0503, -0.0447],\n",
      "          ...,\n",
      "          [-0.0149,  0.0335,  0.0261,  ..., -0.0112, -0.0074,  0.0000],\n",
      "          [-0.0168, -0.0149,  0.0391,  ..., -0.0168, -0.0130, -0.0149],\n",
      "          [ 0.0019, -0.0019, -0.0205,  ..., -0.0447, -0.0298, -0.0317]]],\n",
      "\n",
      "\n",
      "        [[[-0.0186, -0.0261, -0.0354,  ..., -0.0391, -0.1061,  0.0372],\n",
      "          [-0.0112, -0.1173, -0.0950,  ..., -0.0912, -0.0689,  0.0503],\n",
      "          [ 0.0466, -0.0261, -0.0298,  ..., -0.0019,  0.0633, -0.0801],\n",
      "          ...,\n",
      "          [ 0.0205, -0.0521,  0.0130,  ...,  0.0615, -0.0670,  0.0484],\n",
      "          [ 0.0428,  0.0261,  0.0112,  ...,  0.0149, -0.0801,  0.0372],\n",
      "          [-0.0261, -0.0223, -0.0521,  ..., -0.0223, -0.0168,  0.0931]]],\n",
      "\n",
      "\n",
      "        [[[-0.0242,  0.0931,  0.0689,  ...,  0.0577, -0.0149, -0.0615],\n",
      "          [ 0.0819,  0.0428, -0.0708,  ...,  0.0466,  0.0074,  0.0745],\n",
      "          [-0.0037, -0.0652, -0.0615,  ...,  0.0093,  0.0186,  0.1006],\n",
      "          ...,\n",
      "          [ 0.0317, -0.0279, -0.1192,  ...,  0.0577,  0.0186,  0.0391],\n",
      "          [-0.0317,  0.0298, -0.0447,  ...,  0.0354,  0.0186, -0.0466],\n",
      "          [ 0.0745, -0.0019,  0.0521,  ...,  0.0968, -0.0521, -0.0317]]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original weights: ')\n",
    "print(net.conv1.weight)  # Display only a small part of the weights for clarity\n",
    "print('')\n",
    "print('Dequantized weights: ')\n",
    "print(torch.dequantize(model_quantized.conv1.weight()))  # Display only a small part of the dequantized weights for clarity\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print size and accuracy of the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model after quantization\n",
      "Size (KB): 11216.877\n"
     ]
    }
   ],
   "source": [
    "print('Size of the model after quantization')\n",
    "print_size_of_model(model_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model after quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:09<00:00, 106.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing the model after quantization')\n",
    "test(model_quantized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
